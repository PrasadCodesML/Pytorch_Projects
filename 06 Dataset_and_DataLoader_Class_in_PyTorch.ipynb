{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#### **Problems in last notebook**\n",
        "\n",
        "1. We are using Batch Gradient Descent means to update the weights in backward pass we are sending all of our data at once and then changing the weights.\n",
        "2. Batch gradient is memory inefficient\n",
        "3. No better convergence (No fast updation of parameters)\n",
        "\n",
        "Instead of loading entire data at once we will divide the data in say 10 batches update the weights (backward pass) after forward pass of each batch which will result in better convergence and will be memory efficient : This is **Mini Batch Gradient Descent**"
      ],
      "metadata": {
        "id": "eo-MhM3Wv7Ww"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Dataset and DataLoader Classes\n",
        "\n",
        "Dataset and DataLoader are core abstractions in PyTorch that decouple how you define your data from how you efficiently iterate over it in training loops.\n",
        "\n",
        "You `data` is in memory and the `dataset class` know where the data is located and it retrieves the rows of your data from memory one by one now the `dataloader class` takes this loaded data and create batches from them.\n",
        "\n",
        "So the `dataset class` loads the data and the `dataloader class` create batches of this loaded data.\n",
        "\n",
        "**Dataset Class**\n",
        "The dataset class if essentially a blueprint. When you create a custom dataset, you decide how data is loaded and returned.\n",
        "\n",
        "It defines :    \n",
        "  - `__init__()` : which tells how the data should be loaded.\n",
        "  - `__len__()` : which returns the total number of samples\n",
        "  - `__getitem__(index)` : which returns the data (and label) at the given index. Also if you want to do any transformations it should be done in this class (eg. augmentation, resizing, stemming, lemetization, etc)\n",
        "\n",
        "**DataLoader Class**\n",
        "The DataLoader wraps a Dataset and handles batching, shuffling, and parallel loading for you.\n",
        "\n",
        "**DataLoader Control Flow** :\n",
        "  - At the start of each epoch, the DataLoader (if shuffle=True) shuffles indices (using a sampler)\n",
        "  - It divides the indices into chunks of batch_size\n",
        "  - For each index in the chunk, data samples are fetched from the Dataset object\n",
        "  - The samples are then collected and combined into a batch (using collate_fn)\n",
        "  - The batch is returned to the main training loop"
      ],
      "metadata": {
        "id": "kVjUFJHexz0o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Parallelization**\n",
        "\n",
        "If you want to do parallel operation you can do it using DataLoader Class what it does it it assign the batches created to different workers and these workers do the task simultaneously"
      ],
      "metadata": {
        "id": "2kxHwS7_8-hO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Sampler**\n",
        "\n",
        "In PyTorch, the sampler in the DataLoader determines the strategy for selecting samples from the dataset during data loading. It controls how indices of the dataset are drawn for each batch\n",
        "\n",
        "**Types of Sampler**\n",
        "PyTorch provides several predefined samplers, and you can create custom ones:  \n",
        "  1. SequentialSampler :    \n",
        "    - Samples elements sequentially, in the order they appear in the dataset.\n",
        "    - Default when shuffle = False\n",
        "  2. Random Sampler :     \n",
        "    - Samples elements randomly without replacement\n",
        "    - Default when shuffle = True"
      ],
      "metadata": {
        "id": "n6WWn6_c9ESb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `collate_fn`\n",
        "\n",
        "The `collate_fn` in PyTorch's DataLoader is a function that specifies how to combine a list of samples from a dataset into a single batch. By default, the DataLoader uses a simple batch collation mechanism, but `collate_fn` allows you to customize how the data should be processed and batched\n",
        "\n"
      ],
      "metadata": {
        "id": "KAFReFHr_Hej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DataLoader Important Parameters\n",
        "\n",
        "The DataLoader class in PyTorch comes with several parameters that allows you to customize how data is loaded, batched, and preprocessed. Some of the most commonly used and important parameters includes\n",
        "\n",
        "  1. dataset(mandatory) :    \n",
        "    - The Dataset from which the DataLoader will pull data\n",
        "    - Must be a subclass of torch.utils.data.Dataset that implements `__getitem__` and `__len__`\n",
        "  2. batch_size :    \n",
        "    - How many samples per batch to load\n",
        "    - Default is 1\n",
        "    - Larger batch sizes can speed up training on GPUs but require more memory\n",
        "  3. shuffle :    \n",
        "    - If True, the DataLoader will shuffle the dataset incides each epoch\n",
        "  4. num_worker :    \n",
        "    - The number of worker processes used to load data in parallel\n",
        "    - Setting num_workder > 0 can speed up data loading by leverging multiple CPU cores, especially if I/O or preprocessing is a bottelneck\n",
        "  5. pin_memory :    \n",
        "    - If True, the DataLoader will copy tensors into pinned(page-locked) memory before returning them.\n",
        "    - This can improve GPU transfer speed and thun overall training throughput, particularly on CUDA systems\n",
        "  6. drop_last :    \n",
        "    - If True, the DataLoader will drop the last incomplete batch if the total number of samples is not divisble by the batch_size\n",
        "    - Useful when exact batch sizes are required (for eg. in some batch nomalization scenarios)\n",
        "  7. collate_fn :   \n",
        "    - A callable that processes a list of samples into a batch (the default simply stacks tensors).\n",
        "    - Custom collate_fn can handle variable - length sequences, perform custom batching logic, or handle complex data structures\n",
        "  8. sampler :     \n",
        "    - sampler defines the strategy for drawing samples (eg. for handling imbalanced classes, or custom sampling strategies)\n",
        "    - batch_sampler works at the batch level, controlling how batches are formed.\n",
        "    - Typically, you don't need to specify these if you are using batch_size an shuffle. However, they provide lower-level control if you have advanced requirements"
      ],
      "metadata": {
        "id": "sO9WWlZv_9Gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dataset Creation"
      ],
      "metadata": {
        "id": "RdqMWkzm5W_p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KGVFacRIv246"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "from sklearn.datasets import make_classification\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1 : Create a synthetic classification dataset using sklearn\n",
        "X, y = make_classification(\n",
        "    n_samples = 10, # Number of samples\n",
        "    n_features = 2, # Number of features\n",
        "    n_informative = 2, # Number of informative features\n",
        "    n_redundant = 0, # Number of redundant features\n",
        "    n_classes = 2, # Number of classes\n",
        "    random_state = 42\n",
        ")"
      ],
      "metadata": {
        "id": "--sjQRK85ikX"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsWBvaPz6H9A",
        "outputId": "860550dc-b552-4053-e48a-0905c58b5f98"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.06833894, -0.97007347],\n",
              "       [-1.14021544, -0.83879234],\n",
              "       [-2.8953973 ,  1.97686236],\n",
              "       [-0.72063436, -0.96059253],\n",
              "       [-1.96287438, -0.99225135],\n",
              "       [-0.9382051 , -0.54304815],\n",
              "       [ 1.72725924, -1.18582677],\n",
              "       [ 1.77736657,  1.51157598],\n",
              "       [ 1.89969252,  0.83444483],\n",
              "       [-0.58723065, -1.97171753]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLO_c6Bm6INQ",
        "outputId": "1dc97982-0439-45e3-802d-51fa1ffdce74"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10, 2)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lzJV71Q76LLQ",
        "outputId": "b0e08dc5-f93d-4fd3-d715-781cbc1a8827"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 0, 1, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T76wNnD46L8E",
        "outputId": "5c73288e-50ba-4591-e474-2e421bd412d4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Converting NumPy arrays into PyTorch tensors"
      ],
      "metadata": {
        "id": "anytyTLZ6OP2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = torch.tensor(X, dtype = torch.float32)\n",
        "y = torch.tensor(y, dtype = torch.long)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU5g0Ktf6NII",
        "outputId": "b4d7c20f-31c4-4926-b52d-d5da76b28cb8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-93142c411928>:1: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X, dtype = torch.float32)\n",
            "<ipython-input-8-93142c411928>:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  y = torch.tensor(y, dtype = torch.long)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rrDK15F6Zgs",
        "outputId": "a7116fee-4fd5-4d92-b03a-0687f115e3d4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 1.0683, -0.9701],\n",
              "        [-1.1402, -0.8388],\n",
              "        [-2.8954,  1.9769],\n",
              "        [-0.7206, -0.9606],\n",
              "        [-1.9629, -0.9923],\n",
              "        [-0.9382, -0.5430],\n",
              "        [ 1.7273, -1.1858],\n",
              "        [ 1.7774,  1.5116],\n",
              "        [ 1.8997,  0.8344],\n",
              "        [-0.5872, -1.9717]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IKjlVzhn6cH-",
        "outputId": "2c1c5cdc-da98-43ef-cc18-e9e7674877ce"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([1, 0, 0, 0, 0, 1, 1, 1, 1, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating Dataset Class"
      ],
      "metadata": {
        "id": "yUpY8YZD7eUm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "6MDFd5Xd6ce2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataset(Dataset):\n",
        "  def __init__(self, features, labels):\n",
        "    self.features = features\n",
        "    self.labels = labels\n",
        "  def __len__(self):\n",
        "    return self.features.shape[0]\n",
        "  def __getitem__(self, index):\n",
        "    return self.features[index], self.labels[index]"
      ],
      "metadata": {
        "id": "DvT_kgY36h95"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = CustomDataset(X, y)"
      ],
      "metadata": {
        "id": "0WvBnSlc6_PN"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rSw-1d1L7Be-",
        "outputId": "8a707d16-b4c3-487f-b961-e160356d17f6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3MjtA5H7FlB",
        "outputId": "26ed30b4-3c42-4155-dc59-5c410913a7c8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([ 1.0683, -0.9701]), tensor(1))"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[8]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wi763jNC7Gq9",
        "outputId": "8e3fc92b-6d06-4440-db6d-c6c5c2bfe4a0"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([1.8997, 0.8344]), tensor(1))"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating DataLoader class"
      ],
      "metadata": {
        "id": "iHtXNhEQ7hof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataloader = DataLoader(dataset, batch_size = 2, shuffle = True)"
      ],
      "metadata": {
        "id": "F3Y6yhFo7Hzt"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for batch_features, batch_labels in dataloader:\n",
        "  print(batch_features)\n",
        "  print(batch_labels)\n",
        "  print('-'*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6Lhh-6s7P2A",
        "outputId": "fe0506bf-0615-4988-d028-bdd9ae66cc18"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-0.9382, -0.5430],\n",
            "        [ 1.7273, -1.1858]])\n",
            "tensor([1, 1])\n",
            "--------------------------------------------------\n",
            "tensor([[-2.8954,  1.9769],\n",
            "        [-1.1402, -0.8388]])\n",
            "tensor([0, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.8997,  0.8344],\n",
            "        [-0.5872, -1.9717]])\n",
            "tensor([1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[ 1.0683, -0.9701],\n",
            "        [-0.7206, -0.9606]])\n",
            "tensor([1, 0])\n",
            "--------------------------------------------------\n",
            "tensor([[-1.9629, -0.9923],\n",
            "        [ 1.7774,  1.5116]])\n",
            "tensor([0, 1])\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Applying Mini Batch Gradient Descent using Dataset and DataLoader classes on Breast Cancer Project"
      ],
      "metadata": {
        "id": "e1DfWcAdDQRy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Imports"
      ],
      "metadata": {
        "id": "bAR_sQZpEOi4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "66KJvH5d7XL6"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"https://raw.githubusercontent.com/gscdit/Breast-Cancer-Detection/refs/heads/master/data.csv\")"
      ],
      "metadata": {
        "id": "qDOd3EI9DuxF"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocessing, Scaling, Encoding and converting into PyTorch Tensors"
      ],
      "metadata": {
        "id": "z-IFMYSwERaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()\n",
        "df.drop(columns=[\"Unnamed: 32\",\"id\"],inplace=True)"
      ],
      "metadata": {
        "id": "DbofhfyKDyWk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(df.iloc[:,1:],df.iloc[:,0],test_size=0.2,random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "encoder = LabelEncoder()\n",
        "y_train = encoder.fit_transform(y_train)\n",
        "y_test = encoder.transform(y_test)"
      ],
      "metadata": {
        "id": "tYsRzaGFD2WL"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "S2bNGUEpD3AV"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Creating CustomDataset class"
      ],
      "metadata": {
        "id": "3BecF5ozEFWt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]"
      ],
      "metadata": {
        "id": "mz2ruz3YEmDU"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = CustomDataset(X_test_tensor, y_test_tensor)"
      ],
      "metadata": {
        "id": "ix_qplqVEpl6"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset[10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2CYK4k8dEsoz",
        "outputId": "e1c7baf7-406d-415e-e76e-2422513c9225"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([-0.4976,  0.6137, -0.4981, -0.5310, -0.5769, -0.1749, -0.3622, -0.2849,\n",
              "          0.4335,  0.1782, -0.3684,  0.5531, -0.3167, -0.4052,  0.0403, -0.0380,\n",
              "         -0.1804,  0.1648, -0.1217,  0.2308, -0.5004,  0.8194, -0.4692, -0.5331,\n",
              "         -0.0491, -0.0416, -0.1491,  0.0968,  0.1062,  0.4904]),\n",
              " tensor(0.))"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
      ],
      "metadata": {
        "id": "hQmYZCW-Eyn2"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Defining the Model"
      ],
      "metadata": {
        "id": "RkABsPAwE3n-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Defining the Model\n",
        "\n",
        "class Model(nn.Module):\n",
        "  def __init__(self, num_features):\n",
        "    super().__init__()\n",
        "    self.linear1 = nn.Linear(num_features, 3)\n",
        "    self.relu = nn.ReLU()\n",
        "    self.linear2 = nn.Linear(3, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "  def forward(self, features):\n",
        "    out = self.linear1(features)\n",
        "    out = self.relu(out)\n",
        "    out = self.linear2(out)\n",
        "    out = self.sigmoid(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "gfFoUaoJFPJb"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Important Paramters"
      ],
      "metadata": {
        "id": "J8AtfvgnGCw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate = 0.1\n",
        "epochs = 25"
      ],
      "metadata": {
        "id": "2mfdMGd8FSCp"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model(X_train_tensor.shape[1])\n",
        "\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "loss_function = nn.BCELoss()"
      ],
      "metadata": {
        "id": "AMzMmPyxFE3c"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Training"
      ],
      "metadata": {
        "id": "_NgpfgQXGFZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(epochs):\n",
        "  for batch_features, batch_labels in train_loader:\n",
        "    y_pred = model(batch_features)\n",
        "    loss = loss_function(y_pred, batch_labels.view(-1, 1))\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "  print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5aoRhAbFE2i9",
        "outputId": "d3529721-63b9-4956-fc87-55f9834ce025"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 0.5787898302078247\n",
            "Epoch 2, Loss: 0.32075998187065125\n",
            "Epoch 3, Loss: 0.2856273651123047\n",
            "Epoch 4, Loss: 0.2120477259159088\n",
            "Epoch 5, Loss: 0.0909612700343132\n",
            "Epoch 6, Loss: 0.23282615840435028\n",
            "Epoch 7, Loss: 0.25627651810646057\n",
            "Epoch 8, Loss: 0.1917082518339157\n",
            "Epoch 9, Loss: 0.14033189415931702\n",
            "Epoch 10, Loss: 0.18099470436573029\n",
            "Epoch 11, Loss: 0.08367836475372314\n",
            "Epoch 12, Loss: 0.07366105169057846\n",
            "Epoch 13, Loss: 0.6025298237800598\n",
            "Epoch 14, Loss: 0.1660197228193283\n",
            "Epoch 15, Loss: 0.11129027605056763\n",
            "Epoch 16, Loss: 0.08485733717679977\n",
            "Epoch 17, Loss: 0.07812585681676865\n",
            "Epoch 18, Loss: 0.08550560474395752\n",
            "Epoch 19, Loss: 0.1536358743906021\n",
            "Epoch 20, Loss: 0.0030576311983168125\n",
            "Epoch 21, Loss: 0.08321792632341385\n",
            "Epoch 22, Loss: 0.06259900331497192\n",
            "Epoch 23, Loss: 0.2605583667755127\n",
            "Epoch 24, Loss: 0.011140079237520695\n",
            "Epoch 25, Loss: 0.033462852239608765\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaluation"
      ],
      "metadata": {
        "id": "IarrHq2wF-ex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "accuracy_list = []\n",
        "with torch.no_grad():\n",
        "  for batch_features, batch_labels in test_loader:\n",
        "    y_pred = model(batch_features)\n",
        "    y_pred = (y_pred > 0.5).float()\n",
        "    batch_accuracy = (y_pred.view(-1) == batch_labels).float().mean()\n",
        "    accuracy_list.append(batch_accuracy.item())\n",
        "\n",
        "overall_accuracy = sum(accuracy_list) / len(accuracy_list)\n",
        "print(f\"Accuracy : {overall_accuracy}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LctYAXcGE9S8",
        "outputId": "4b115b47-9bf5-4342-87b4-45c09b37cb11"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 0.9782986044883728\n"
          ]
        }
      ]
    }
  ]
}